## เปรียบเทียบผลลัพธ์จากการทดลองด้วย Hidden layers จำนวน 3 layers
![image](https://user-images.githubusercontent.com/83268624/152385817-d8c6a0c5-f552-4034-b867-26c16e4e5a05.png)
 Batch size: พบว่า ยิ่ง Batch ยิ่งมาก ค่า Loss % และ AUC จะไม่ต่างกันมากแต่พบว่า ที่ Batch size ที่สูงขึ้นจะทำให้เวลาเฉลี่ยในการคำนวณลดลงอย่างเห็นได้ชัดอีกทั้งโดยที่ Batch Size 1024 จะให้ค่าเฉลี่ยของ % AUC สูงสุดที่ 93% ค่า Loss เฉลี่ยที่ 0.170 และเวลาในการประมวลผลเฉลี่ยของการ Train อยู่ที่ 43.127 วินาทีซึ่งเป็นเวลาที่น้อยที่สุด

Epoch: จากการวิเคราะห์พบว่า จำนวน Epoch มี   % AUC และ Loss ที่ไม่ต่างกัน แต่ Running Time ที่ใช้กลับมากขึ้นเมื่อเพิ่มจำนวณ Epoch โดยที่  25 Epoch จะให้ % AUC สูงสุดที่ 93 % ในขณะที่ 10 Epoch ให้ค่าเฉลี่ย %AUC ที่ 91.7 % แต่ค่าเฉลี่ยเวลาในการประมวลผลการ Train อยู่ที่ 42.966 วินาทีซึ่งเร็วกว่า  25 Epoch อยู่ถึง 2.3 เท่า

Learning rate: % AUC และ Loss,  Running Time ที่ไม่ต่างกัน แต่ในการทดลองพบว่าที่ ใช้ Learning 0.001 จะให้ % AUC 92.1%   และค่า Loss 0.155 ซึ่งดีกว่า Learning Rate ที่0.005

 Optimizer: จากผลการทดลองพบว่าการใช้ Optimizer เป็น sgd ได้ผลเฉลี่ยทั้ง AUC, Loss และเวลาการ Train ดีกว่าการใช้ Optimizer เป็น adum ค่อนข้างมาก โดย sgd ได้ AUC เฉลี่ย 92.5 % และ Loss เฉลี่ย 0.289 ในขณะที่ adum มี % AUC เฉลี่ยเพียง 91.13%  และ Loss ที่ 0.289

Number of nodes: จากผลการทดลองพบว่าการใช้จำนวน nodes ใน Hidden layer เท่ากับ 28 และ 56 ทำให้ ได้ค่าเฉลี่ย% AUC และ Loss ดีกว่าจำนวน nodes เท่ากับ 1 อย่างชัดเจนแม้ว่าจานวน nodes เท่ากับ 56 จะทำให้ได้ AUC และ Loss เฉลี่ยดีกว่าที่ 96.3 % และ 0.029 ในขณะที่จำนวน nodes เท่ากับ 28 ก็สามารถทำได้ใกล้เคียงกันที่ 96.1% และ 0.037 


Activation: จากผลการทดลองพบว่าการใช้ Activation เป็น sigmoid ให้ค่าเฉลี่ย AUC ดีกว่าการใช้ Activation เป็น relu อย่างมากโดย sigmoid ได้ค่าเฉลี่ย AUC ที่ 96.7 % ในขณะที่ relu ได้เพียง 87.5% เท่านั้น 

จากผลการทดลองจานวน 216 ผลการทดลองพบว่า การทดลองที่ให้ผล AUC, Loss และระยะเวลาการ Train ดี ที่สุดเป็นดังนี้

![image](https://user-images.githubusercontent.com/83268624/152386737-dc0e9534-31fc-4841-9c53-6414b87c9e61.png)



